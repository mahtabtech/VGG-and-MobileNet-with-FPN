{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM778jgI1D1Oj6BipHKfEn0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahtabtech/VGG-and-MobileNet-with-FPN/blob/main/VGG_and_MobileNet_with_FPN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating Feature Pyramid Networks (FPNs) using two other popular models: VGG-16 and MobileNetV2.**"
      ],
      "metadata": {
        "id": "N7crk4FI53Zi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FPN with VGG-16**"
      ],
      "metadata": {
        "id": "JUJ7-2Sr58Wi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDru0KMI5PxJ",
        "outputId": "c9a55e32-367e-4e86-94c2-c6f858e7c787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 112, 112]) torch.Size([1, 128, 56, 56]) torch.Size([1, 512, 28, 28])\n",
            "torch.Size([1, 256, 112, 112]) torch.Size([1, 256, 56, 56]) torch.Size([1, 256, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "## Feature Pyramid Network (FPN) using VGG-16\n",
        "\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load the pre-trained VGG-16 model with the correct weights syntax\n",
        "model_vgg = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)\n",
        "\n",
        "# Extract feature maps from different layers of VGG-16\n",
        "# VGG-16 has a simpler, sequential layer structure\n",
        "# Shallow layer feature map (up to the 5th layer)\n",
        "layer1_vgg = nn.Sequential(*list(model_vgg.features)[:5])  # Early stage feature map\n",
        "\n",
        "# Middle layer feature map (up to the 10th layer)\n",
        "layer2_vgg = nn.Sequential(*list(model_vgg.features)[:10])  # Middle stage feature map\n",
        "\n",
        "# Deepest layer feature map (up to the 23rd layer, including all convolutional layers)\n",
        "layer3_vgg = nn.Sequential(*list(model_vgg.features)[:23])  # Late stage feature map\n",
        "\n",
        "# Example input tensor (e.g., a 224x224 image)\n",
        "x_vgg = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Get feature maps from the different layers\n",
        "feat1_vgg = layer1_vgg(x_vgg)  # Early stage feature map\n",
        "feat2_vgg = layer2_vgg(x_vgg)  # Middle stage feature map\n",
        "feat3_vgg = layer3_vgg(x_vgg)  # Late stage feature map\n",
        "\n",
        "# Print the shapes of the feature maps\n",
        "print(feat1_vgg.shape, feat2_vgg.shape, feat3_vgg.shape)\n",
        "\n",
        "# Define the Feature Pyramid Network (FPN) for VGG-16\n",
        "class FPN_VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FPN_VGG, self).__init__()\n",
        "\n",
        "        # 1x1 convolutions to reduce the number of channels to 256\n",
        "        self.conv3 = nn.Conv2d(512, 256, kernel_size=1)  # For the deepest feature map\n",
        "        self.conv2 = nn.Conv2d(128, 256, kernel_size=1)  # For the middle feature map\n",
        "        self.conv1 = nn.Conv2d(64, 256, kernel_size=1)   # For the shallowest feature map\n",
        "\n",
        "        # 3x3 convolution for further refinement\n",
        "        self.conv_out = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, feat1, feat2, feat3):\n",
        "        # Convert feat3 to 256 channels (deepest feature map)\n",
        "        p3 = self.conv3(feat3)\n",
        "\n",
        "        # Upsample p3 and merge with the middle stage feature map (feat2)\n",
        "        p3_upsampled = F.interpolate(p3, size=(feat2.shape[2], feat2.shape[3]), mode='nearest')\n",
        "        p2 = self.conv2(feat2) + p3_upsampled\n",
        "        p2 = self.conv_out(p2)\n",
        "\n",
        "        # Upsample p2 and merge with the early stage feature map (feat1)\n",
        "        p2_upsampled = F.interpolate(p2, size=(feat1.shape[2], feat1.shape[3]), mode='nearest')\n",
        "        p1 = self.conv1(feat1) + p2_upsampled\n",
        "        p1 = self.conv_out(p1)\n",
        "\n",
        "        return p1, p2, p3\n",
        "\n",
        "# Instantiate and run the FPN for VGG-16\n",
        "fpn_vgg = FPN_VGG()\n",
        "p1_vgg, p2_vgg, p3_vgg = fpn_vgg(feat1_vgg, feat2_vgg, feat3_vgg)\n",
        "\n",
        "# Print the shapes of the output feature maps\n",
        "print(p1_vgg.shape, p2_vgg.shape, p3_vgg.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FPN with MobileNet**"
      ],
      "metadata": {
        "id": "4JR6dKbr656-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Feature Pyramid Network (FPN) using MobileNetV2\n",
        "\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load the pre-trained MobileNetV2 model with the correct weights syntax\n",
        "model_mobilenet = torchvision.models.mobilenet_v2(weights=torchvision.models.MobileNet_V2_Weights.DEFAULT)\n",
        "\n",
        "# Extract feature maps from different stages of MobileNetV2\n",
        "# Shallow layer feature map (up to the 4th bottleneck)\n",
        "layer1_mobilenet = nn.Sequential(*list(model_mobilenet.features)[:4])\n",
        "\n",
        "# Middle layer feature map (up to the 7th bottleneck)\n",
        "layer2_mobilenet = nn.Sequential(*list(model_mobilenet.features)[:7])\n",
        "\n",
        "# Deepest layer feature map (up to the final bottleneck)\n",
        "layer3_mobilenet = nn.Sequential(*list(model_mobilenet.features)[:14])\n",
        "\n",
        "# Example input tensor (e.g., a 224x224 image)\n",
        "x_mobilenet = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Get feature maps from the different layers\n",
        "feat1_mobilenet = layer1_mobilenet(x_mobilenet)  # Early stage feature map\n",
        "feat2_mobilenet = layer2_mobilenet(x_mobilenet)  # Middle stage feature map\n",
        "feat3_mobilenet = layer3_mobilenet(x_mobilenet)  # Late stage feature map\n",
        "\n",
        "# Print the shapes of the feature maps\n",
        "print(feat1_mobilenet.shape, feat2_mobilenet.shape, feat3_mobilenet.shape)\n",
        "\n",
        "# Define the Feature Pyramid Network (FPN) for MobileNetV2\n",
        "class FPN_MobileNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FPN_MobileNet, self).__init__()\n",
        "\n",
        "        # 1x1 convolutions to reduce the number of channels to 256\n",
        "        self.conv3 = nn.Conv2d(96, 256, kernel_size=1)   # For the deepest feature map\n",
        "        self.conv2 = nn.Conv2d(32, 256, kernel_size=1)   # For the middle feature map\n",
        "        self.conv1 = nn.Conv2d(24, 256, kernel_size=1)   # For the shallowest feature map\n",
        "\n",
        "        # 3x3 convolution for further refinement\n",
        "        self.conv_out = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, feat1, feat2, feat3):\n",
        "        # Convert feat3 to 256 channels (deepest feature map)\n",
        "        p3 = self.conv3(feat3)\n",
        "\n",
        "        # Upsample p3 and merge with the middle stage feature map (feat2)\n",
        "        p3_upsampled = F.interpolate(p3, size=(feat2.shape[2], feat2.shape[3]), mode='nearest')\n",
        "        p2 = self.conv2(feat2) + p3_upsampled\n",
        "        p2 = self.conv_out(p2)\n",
        "\n",
        "        # Upsample p2 and merge with the early stage feature map (feat1)\n",
        "        p2_upsampled = F.interpolate(p2, size=(feat1.shape[2], feat1.shape[3]), mode='nearest')\n",
        "        p1 = self.conv1(feat1) + p2_upsampled\n",
        "        p1 = self.conv_out(p1)\n",
        "\n",
        "        return p1, p2, p3\n",
        "\n",
        "# Instantiate and run the FPN for MobileNetV2\n",
        "fpn_mobilenet = FPN_MobileNet()\n",
        "p1_mobilenet, p2_mobilenet, p3_mobilenet = fpn_mobilenet(feat1_mobilenet, feat2_mobilenet, feat3_mobilenet)\n",
        "\n",
        "# Print the shapes of the output feature maps\n",
        "print(p1_mobilenet.shape, p2_mobilenet.shape, p3_mobilenet.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB2KI3ZM6j97",
        "outputId": "aa81efe7-0b6b-448c-99e3-0834a24424d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 183MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 24, 56, 56]) torch.Size([1, 32, 28, 28]) torch.Size([1, 96, 14, 14])\n",
            "torch.Size([1, 256, 56, 56]) torch.Size([1, 256, 28, 28]) torch.Size([1, 256, 14, 14])\n"
          ]
        }
      ]
    }
  ]
}